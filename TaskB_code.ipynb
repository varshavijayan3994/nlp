{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data, it returns a dictionary type, in which there are three keys: id, body and keyphrases. The values corresponds to 2000 scientific articles. The code is tested under python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data_body_keyphrases.json', encoding='utf-8')\n",
    "#data is a dic type\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'body', 'keyphrases'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docids = data['id']\n",
    "len(docids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data['body']\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases = data['keyphrases']\n",
    "len(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs[:3]\n",
    "keyphrases = keyphrases[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B task: Develop your unsupervised keyword extraction model(s) here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Varsha\n",
      "[nltk_data]     Vijayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases_list = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#iterate over every article in the data\n",
    "for article in docs:\n",
    "\n",
    "    def clean(text):\n",
    "        #lowercase words\n",
    "        text = text.lower()\n",
    "        #url removal\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        #filter all sets of punctuation, digits, ascii_letters and whitespace.\n",
    "        printable = set(string.printable)\n",
    "        text = filter(lambda x: x in printable, text)\n",
    "        text = \"\".join(list(text))\n",
    "        return text\n",
    "\n",
    "    #pass each article for text pre processing\n",
    "    Cleaned_text = clean(article)\n",
    "    \n",
    "    #tokenize the clean article\n",
    "    text = word_tokenize(Cleaned_text)\n",
    "\n",
    "    #get POS tags for the generated tokens\n",
    "    POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "    \n",
    "    adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "    lemmatized_text = []\n",
    "    \n",
    "    #map appropriate POS tags to the tokens and lemmatize them\n",
    "    for word in POS_tag:\n",
    "        if word[1] in adjective_tags:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "        else:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "    \n",
    "    \n",
    "    POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "\n",
    "\n",
    "    stopwords_list = []\n",
    "\n",
    "    wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "    #remove words no in the above POS tag as they are irrelevant\n",
    "    for word in POS_tag:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords_list.append(word[0])\n",
    "\n",
    "    punctuations = list(str(string.punctuation))\n",
    "    \n",
    "    stopwords_list = stopwords_list + punctuations\n",
    "\n",
    "\n",
    "    stops = stopwords.words('english')\n",
    "\n",
    "    # add the general stop words from nltk library to the list of stopwords\n",
    "    lots_of_stopwords = stopwords\n",
    "\n",
    "\n",
    "    stopwords_plus = []\n",
    "    stopwords_plus = stopwords_list + stops\n",
    "\n",
    "\n",
    "    stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "\n",
    "    processed_text = []\n",
    "    for word in lemmatized_text:\n",
    "        if word not in stopwords_plus:\n",
    "            processed_text.append(word)\n",
    "    \n",
    "    #get unique set of vocabulary\n",
    "    vocabulary = list(set(processed_text))\n",
    "\n",
    "\n",
    "    vocab_len = len(vocabulary)\n",
    "\n",
    "    weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "    score = np.zeros((vocab_len),dtype=np.float32)\n",
    "    window_size = 3\n",
    "    covered_coocurrences = []\n",
    "\n",
    "    for i in range(0,vocab_len):\n",
    "        score[i]=1\n",
    "        for j in range(0,vocab_len):\n",
    "            if j==i:\n",
    "                #assign score as 0 if the same word is compared against one another\n",
    "                weighted_edge[i][j]=0\n",
    "            else:\n",
    "                for window_start in range(0,(len(processed_text)-window_size)):\n",
    "\n",
    "                    window_end = window_start+window_size\n",
    "\n",
    "                    window = processed_text[window_start:window_end]\n",
    "\n",
    "                    if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "\n",
    "                        index_of_i = window_start + window.index(vocabulary[i])\n",
    "                        index_of_j = window_start + window.index(vocabulary[j])\n",
    "\n",
    "\n",
    "                        if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                            weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                            covered_coocurrences.append([index_of_i,index_of_j])\n",
    "\n",
    "\n",
    "    inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "    for i in range(0,vocab_len):\n",
    "        for j in range(0,vocab_len):\n",
    "            inout[i]+=weighted_edge[i][j]\n",
    "\n",
    "\n",
    "    MAX_ITERATIONS = 50\n",
    "    #set damping factor as 0.85\n",
    "    #score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertieces that has a connection with i.\n",
    "    d=0.85\n",
    "    threshold = 0.0001 #convergence threshold\n",
    "\n",
    "    for iter in range(0,MAX_ITERATIONS):\n",
    "        prev_score = np.copy(score)\n",
    "\n",
    "        for i in range(0,vocab_len):\n",
    "\n",
    "            summation = 0\n",
    "            for j in range(0,vocab_len):\n",
    "                if weighted_edge[i][j] != 0:\n",
    "                    summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "\n",
    "            score[i] = (1-d) + d*(summation)\n",
    "\n",
    "        if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    phrases = []\n",
    "\n",
    "    phrase = \" \"\n",
    "    for word in lemmatized_text:\n",
    "\n",
    "        if word in stopwords_plus:\n",
    "            if phrase!= \" \":\n",
    "                phrases.append(str(phrase).strip().split())\n",
    "            phrase = \" \"\n",
    "        elif word not in stopwords_plus:\n",
    "            phrase+=str(word)\n",
    "            phrase+=\" \"\n",
    "\n",
    "\n",
    "\n",
    "    unique_phrases = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if phrase not in unique_phrases:\n",
    "            unique_phrases.append(phrase)\n",
    "\n",
    "\n",
    "    for word in vocabulary:\n",
    "    #print word\n",
    "        for phrase in unique_phrases:\n",
    "            if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "                #if len(phrase)>1 then the current phrase is multi-worded.\n",
    "                #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
    "                # and at the same time present as a word within a multi-worded phrase,\n",
    "                # then I will remove the single-word-phrase from the list.\n",
    "                unique_phrases.remove([word])\n",
    "\n",
    "\n",
    "\n",
    "    phrase_scores = []\n",
    "    keywords = []\n",
    "    for phrase in unique_phrases:\n",
    "        phrase_score=0\n",
    "        keyword = ''\n",
    "        for word in phrase:\n",
    "            keyword += str(word)\n",
    "            keyword += \" \"\n",
    "            phrase_score+=score[vocabulary.index(word)]\n",
    "        phrase_scores.append(phrase_score)\n",
    "        keywords.append(keyword.strip())\n",
    "\n",
    "\n",
    "    sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "    keywords_num = 10\n",
    "    key_phrases = []\n",
    "\n",
    "    for i in range(0,keywords_num):\n",
    "        key_phrases.append(keywords[sorted_index[i]])\n",
    "\n",
    "    #all the key phrases calculated above are appended to the final list of key phrases which is to be evaluated\n",
    "    #against the author assigned key phrases.\n",
    "    key_phrases_list.append(key_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do evaluation for your model here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the following function to get the F score of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following piece of code can help you do the evaluations, \n",
    "i.e. get Fscore for your predictions.\n",
    "'''\n",
    "def compare(golden_words, system_words):\n",
    "    golden_words_flatten = list()\n",
    "    system_words_flatten = list()\n",
    "    i=0\n",
    "    score =0\n",
    "    if len(system_words)>0 and len(golden_words)>0:\n",
    "        for tokenx in golden_words:\n",
    "            golden_words_flatten = golden_words_flatten + str(tokenx).lower().split()\n",
    "        for tokeny in system_words:\n",
    "            system_words_flatten = system_words_flatten + str(tokeny).lower().split()\n",
    "        for word in system_words_flatten:\n",
    "            if word in golden_words_flatten:\n",
    "                i=i+1\n",
    "        recall = i/len(golden_words_flatten)\n",
    "        precision = i/len(system_words_flatten)\n",
    "        if recall>0 or precision>0:\n",
    "            score = 2*precision*recall/(precision + recall)\n",
    "    else:\n",
    "        score = 0\n",
    "    return score\n",
    "            \n",
    "def get_Fscore(golden_list, system_list):\n",
    "    score_sum = 0\n",
    "    num_docs = len(golden_list)\n",
    "    if len(golden_list)==len(system_list):\n",
    "        for i in range(0,num_docs):\n",
    "            score_temp = compare(golden_list[i], system_list[i])\n",
    "            score_sum = score_sum + score_temp\n",
    "    else:\n",
    "        print('Make sure the number of documents is correct !')\n",
    "    return score_sum/num_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author assigned key words\n",
    "golden_words = keyphrases\n",
    "\n",
    "#keywords generated using TestRank algorithm\n",
    "system_words = key_phrases_list\n",
    "print(get_Fscore(golden_words, system_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
